# Local LLM Chat UI
## Basic Usage
This repo uses Ollama, which should be [downloaded](https://ollama.com/download) for your OS.
Install the requirements:
```
pip install -r requirements.txt
```
run the app:
```
streamlit run app.py
```

## Contents
Using a local LLM of your choice you can perform the usual chat, obtain help with code etc. 
You can also use the RAG page to generate responses based on the text.