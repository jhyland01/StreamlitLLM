# Local LLM Chat UI
## Basic Usage
This repo uses Ollama, which should be [downloaded](https://ollama.com/download) for your OS.
Install the requirements:
```
pip install -r requirements.txt
```
run the app:
```
streamlit run app.py
```